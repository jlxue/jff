Phases:
    map -> sort [->combine] -> partition -> shuffle -> merge -> reduce

input data type:
    Key -> WritableComparable
    Value -> Writable

        BooleanWritable, ByteWritable, DoubleWritable, FloatWritable,
        IntWritable, LongWritable, Text, NullWritable

mapper: extends MapReduceBase implements Mapper
    IdentityMapper, InverseMapper, RegexMaper, TokenCountMapper

reducer: extends MapReduceBase implements Reducer
    IdentityReducer, LongSumReducer

parititioner: implements Partitioner
    HashPartitioner

combiner: local reducer

input format:   implements InputFormat
    TextInputFormat, KeyValueTextInputFormat, SequenceFileInputFormat,
    NLineInputFormat

customized input format: extends FileInputFormat
    wrap LineRecordReader or KeyValueLineRecordReader

output format: implements OutputFormat
    extends FileOutputFormat

    TextOutputFormat, SequenceFileOutputFormat, NullOutputFormat

GenericOptionsParser
    -conf <configuration file>
    -D <property=value>
    -fs <local|namenode:port>
    -jt <local|jobtracker:port>
    -files <list of files>
    -libjars <list of jars>
    -archives <list of archives>

See http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html
    -Dmapred.reduce.tasks=0
    -Dmapreduce.job.reduces=0       // new name

    -Dmapred.reduce.tasks=1
    -Dmapreduce.job.reduces=1       // new name

    -Dmapred.map.tasks=4
    -Dmapreduce.job.maps=4          // new name

    -Dkey.value.separator.in.input.line=,
    -Dmapreduce.input.keyvaluelinerecordreader.key.value.separator=,

Hadoop Streaming
    $ hadoop jar share/hadoop/tools/lib/hadoop-streaming-*.jar \
        -input xxx.txt  \
        -output out     \
        -Dmapred.job.name "MyJob" \
        -file some-script   \
        -mapper "...shell command..."   \
        -reducer "...shell command..." | aggregate

    aggregate: http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/aggregate/package-summary.html
        reducer outputs "function:key\tvalue", eg.  "DoubleValueSum:33  55.66"

Chain map-reduce jobs:
    (1) sequential: multiple Job objects, one by one
    (2) dependency based: Job.addDependingJob(Job other)
                          JobControl.addJob();
                          JobControl.run();
                          JobControl.allFinished();
                          JobControl.getFailedJobs();
    (3) preprocess and postproprocess in single job
        ChainMapper.addMapper(...);
        ChainMapper.addMapper(...);
        ...
        ChainReducer.setReducer(...);
        ChainReducer.addMapper(...);
        ChainReducer.addMapper(...);
        ...

Join data:
    (1) share/hadoop/tools/lib/hadoop-datajoin-0.23.5.jar
        reduce-side join, repartitioned sort-merge join

        Three abstract classes:
            DataJoinMapperBase
            DataJoinReducerBase
            TaggedMapOutput

    (2) org.apache.hadoop.mapreduce.lib.join
        map-side join

    (3) org.apache.hadoop.filecache.DistributedCache
        replicated join,  DistributedCache-based join

        hadoop jar -files ... -libjars ... -archives ...

    (4) semi-join: filter on map side with a small group key list file
    accessed by DistributedCache, or with pre-generated bloom filter,
    then join on reduce side

        bloom filter: perhaps false positive
            minimize fale positive rate: k = ln2 * (m/n) = 0.7 * (m/n)
            false rate = 0.6185 ^ (m / n)
                m: count of bits
                n: count of items

        org.apache.hadoop.util.bloom.BloomFilter
                    
Kill job:
    $ hadoop job -kill <job_id>

Report:
    streaming:    reporter:status:<message>
                  reporter:counter:<group>,<counter>,<amount>

    Java:   Reporter.incrCounter(String group, String counter, long amount)
            Reporter.incCounter(Enum key, long amount)

Skip bad records:
    SkipBadRecords.setMapperMaxSkipRecords(...)     -Dmapred.skip.map.max.skip.records=xx
    SkipBadRecords.setReducerMaxSkipGroups(...)     -Dmapred.skip.reduce.max.skip.groups=xx

    JobConf.setMaxMapAttempts()         -Dmapred.map.max.attempts=xx
    JobConf.setMaxReduceAttempts()      -Dmapred.reduce.max.attempts=xx

    SkipBadRecords.setAttemptsToStartSkipping(..)       -Dmapred.skip.attempts.to.start.skipping=xx

    SkipBadRecords.setSkipOutputPath(...)               -Dmapred.skip.out.dir=xx

    skipped records are put to _log/skip/ directory, use "hadoop fs -text <filepath>" to read them

    SkipBadRecords.setAutoIncrMapperProcCount(..)       -Dmapred.skip.map.auto.incr.proc.count=false
    SkipBadRecords.setAutoIncReducerProcCount(..)       -Dmapred.skip.reduce.auto.incr.proc.count=false
        streaming:  reporter:counter:SkippingTaskCounters,MapProcessedRecords,1
                    reporter:counter:SkippingTaskCounters,ReduceProcessedGroups,1
        Java:       reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1)
                    reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1)

IsolationRunner:
    -Dkeep.failed.tasks.files=true

    some-node$ cd ${mapred.local.dir}/taskTracker/jobcache/<job_id>/<attempt_id>/work
    some-node$ export HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,address=8000"
    some-node$ bin/hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml
    some-node$ jdb -attach 8000

Compress mapper's output:
    -Dmapred.compress.map.output=true
    -Dmapred.map.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec

Sequence file:
    Java:
        conf.setInputFormat(SequenceFileInputFormat.class);
        conf.setOutputFormat(SequenceFileOutputStream.class);
        SequenceFileOutputFormat.setOutputCompressionType(conf, CompressionType.BLOCK);
        FileOutputFormat.setCompressOutput(conf, true);
        FileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class);

    streaming:
        -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat
        -inputformat org.apache.hadoop.mapred.SequenceFileInputFormat
        -Dmapred.output.compression.type=BLOCK
        -Dmapred.output.compress=true
        -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec

Reuse JVM for multiple tasks in same job:
    -Dmapred.job.reuse.jvm.num.tasks=<number>
    useful to avoid initialization overhead when use DistributedCache

Speculative execution:
    Should turn off it when tasks have side effect, such as writing external files

    -Dmapred.map.tasks.speculative.execution=false
    -Dmapred.map.tasks.speculative.execution=false

